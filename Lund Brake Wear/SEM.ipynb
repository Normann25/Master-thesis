{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import linecache\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "import periodictable\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "from NFA_Aerosol import Utility_Lib as UL\n",
    "from NFA_Aerosol import Instrument_Lib as IL\n",
    "sys.path.append('..')\n",
    "from read_data_functions import *\n",
    "from plot_functions import *\n",
    "from calculations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'NFA_Aerosol.Utility_Lib' has no attribute 'Partector_TEM_sampling'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 44\u001b[0m\n\u001b[0;32m     41\u001b[0m partector \u001b[38;5;241m=\u001b[39m read_partector(path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPartector/\u001b[39m\u001b[38;5;124m'\u001b[39m, parent_path, partector_keys)\n\u001b[0;32m     42\u001b[0m combined_Partector \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(partector\u001b[38;5;241m.\u001b[39mvalues(), ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 44\u001b[0m durations,starts \u001b[38;5;241m=\u001b[39m UL\u001b[38;5;241m.\u001b[39mPartector_TEM_sampling(combined_Partector)\n\u001b[0;32m     46\u001b[0m sampled_air \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(durations)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m450\u001b[39m \u001b[38;5;66;03m# cm3\u001b[39;00m\n\u001b[0;32m     48\u001b[0m Img_area_mm2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8192\u001b[39m \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m10.2\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1e-6\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m5632\u001b[39m \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m10.2\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1e-6\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'NFA_Aerosol.Utility_Lib' has no attribute 'Partector_TEM_sampling'"
     ]
    }
   ],
   "source": [
    "def read_SEM(path, parent_path):\n",
    "    ParentPath = os.path.abspath(parent_path)\n",
    "    if ParentPath not in sys.path:\n",
    "        sys.path.insert(0, ParentPath)\n",
    "    \n",
    "    files = os.listdir(path)\n",
    "\n",
    "    dfs = []\n",
    "    for file in files:\n",
    "        if file.endswith('.xlsx'):\n",
    "            file_path = os.path.join(path, file)\n",
    "            temp = pd.read_excel(file_path)\n",
    "\n",
    "            # Extracting sample name from the directory structure\n",
    "            sample_name = os.path.basename(os.path.dirname(file_path))\n",
    "            temp[\"Sample\"] = [sample_name] * temp.shape[0]\n",
    "\n",
    "            dfs.append(temp)\n",
    "\n",
    "    if not dfs:\n",
    "        return pd.DataFrame()  # Return an empty DataFrame if no files were processed\n",
    "\n",
    "    # Combine all DataFrames\n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # Unique sample mapping\n",
    "    unique_samples = combined_df['Sample'].unique()\n",
    "    mapping = {original: f\"Sample {i+1}\" for i, original in enumerate(unique_samples)}\n",
    "    combined_df['Sample'] = combined_df['Sample'].map(mapping)\n",
    "\n",
    "    # Drop specified columns if they exist\n",
    "    combined_df = combined_df.drop(columns=[\"Field\", \"Stage X\", \"Stage Y\", \"Id\"], errors='ignore')\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "parent_path = '../../../../'\n",
    "path = 'L:/PG-Nanoteknologi/PROJEKTER/2024 Laura og Nan/Lund/'\n",
    "\n",
    "SEM = read_SEM(path + 'SEM/', parent_path)\n",
    "partector_keys = ['101024_20241016', '142542_20241016', '091255_20241017', '131923_20241017']\n",
    "partector = read_partector(path + 'Partector/', parent_path, partector_keys)\n",
    "combined_Partector = pd.concat(partector.values(), ignore_index=True)\n",
    "\n",
    "durations,starts = UL.Partector_TEM_sampling(combined_Partector)\n",
    "\n",
    "sampled_air = np.array(durations)*450 # cm3\n",
    "\n",
    "Img_area_mm2 = 8192 * (10.2 * 1e-6) * 5632 * (10.2 * 1e-6)\n",
    "Img_number = 5\n",
    "\n",
    "TEM_grid_area_mm2 = np.pi * (3.05/2)**2\n",
    "TEM_grid_area_mm2 = np.pi * (2.9/2)**2\n",
    "\n",
    "Fraction_area_analyzed = (Img_area_mm2*Img_number) / TEM_grid_area_mm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_bins = np.array([  10.  ,  13.45,  17.95,  23.95,  31.95,  42.6 ,  56.8 ,  75.75,\n",
    "101.05, 134.75, 179.7 , 239.6, 300.,   374.,   465.,   579.,   721.,   897.,  1117.,  1391.,\n",
    "1732.,  2156.,  2685.,  3343.,  4162.,  5182.,  6451.,  8031., 10000.])\n",
    "\n",
    "width = size_bins[1:]-size_bins[:-1]\n",
    "Mids = width/2 + size_bins[:-1]\n",
    "\n",
    "log_bin_edges = np.log10(size_bins)\n",
    "dlogDP = log_bin_edges[1:] - log_bin_edges[:-1]\n",
    "\n",
    "Partector_correction = UL.Partector_Ceff(Mids)\n",
    "\n",
    "size_labels = np.arange(size_bins.shape[0]-1)\n",
    "\n",
    "SEM[\"Size Category\"] = pd.cut(SEM[\"ECD (Î¼m)\"] * 1000, bins=size_bins, labels=size_labels)\n",
    "\n",
    "# Define number of samples\n",
    "samples = SEM[\"Sample\"].unique()\n",
    "num_samples = len(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background_elements = ['C Wt%', 'O Wt%', 'Cu Wt%', \"Al Wt%\", \"F Wt%\",\"Eu Wt%\",\"Mn Wt%\",\"Ba Wt%\",\"Os Wt%\",\"Yb Wt%\",\"Tb Wt%\",\"P Wt%\",\"Ta Wt%\",\"Mo Wt%\"]\n",
    "\n",
    "# Identify element columns (atomic % data)\n",
    "element_columns = [col for col in SEM.columns if col.endswith(\" Wt%\") and col not in background_elements]\n",
    "\n",
    "# Identify non-element columns (to keep)\n",
    "non_element_columns = [col for col in SEM.columns if col not in element_columns + background_elements]\n",
    "\n",
    "# Compute total sum of non-background elements per row\n",
    "total_remaining = SEM[element_columns].sum(axis=1)\n",
    "\n",
    "# Create a mask for rows where only background elements are present\n",
    "background_only_mask = total_remaining == 0\n",
    "\n",
    "# Renormalize: Scale remaining elements so that their sum becomes 100%\n",
    "df_normalized = SEM[element_columns].div(total_remaining, axis=0) * 100\n",
    "\n",
    "# Apply the mask: If only background elements were present, set values to NaN\n",
    "df_normalized[background_only_mask] = float(\"nan\")\n",
    "\n",
    "# Merge back with original non-elemental columns (Size, Shape, etc.)\n",
    "df_final = pd.concat([SEM[non_element_columns], df_normalized], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamically fetch atomic masses using `periodictable`\n",
    "def get_atomic_mass(element):\n",
    "    try:\n",
    "        return periodictable.elements.symbol(element).mass\n",
    "    except KeyError:\n",
    "        raise ValueError(f\"Element {element} not found in periodictable\")\n",
    "        \n",
    "# Calculate At% for each particle (row)\n",
    "for col in element_columns:\n",
    "    element = col.split(\" \")[0]  # Extract the element name (e.g., \"Fe\" from \"Fe Wt%\")\n",
    "    atomic_mass = get_atomic_mass(element)\n",
    "    # Calculate molar ratio (Wt% / Atomic Mass)\n",
    "    df_final[f\"{element} Molar Ratio\"] = df_final[col] / atomic_mass\n",
    "\n",
    "# Sum all molar ratios for normalization\n",
    "df_final[\"Total Molar Ratio\"] = df_final[[f\"{col.split()[0]} Molar Ratio\" for col in element_columns]].sum(axis=1)\n",
    "\n",
    "# Calculate At% for each element\n",
    "for col in element_columns:\n",
    "    element = col.split(\" \")[0]\n",
    "    df_final[f\"{element} At%\"] = (\n",
    "        df_final[f\"{element} Molar Ratio\"] / df_final[\"Total Molar Ratio\"] * 100\n",
    "    )\n",
    "\n",
    "# Drop intermediate molar ratio columns if no longer needed\n",
    "df_final = df_final.drop(columns=[f\"{col.split()[0]} Molar Ratio\" for col in element_columns] + [\"Total Molar Ratio\"])\n",
    "element_columns = [col for col in df_final.columns if col.endswith(\" At%\") and col not in background_elements]\n",
    "\n",
    "df_final = df_final.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mean composition for each sample\n",
    "df_avg_composition = df_final.groupby(\"Sample\")[element_columns].mean().reset_index()\n",
    "\n",
    "# Normalize the atomic percentages so they sum to 100%\n",
    "total_sum = df_avg_composition[element_columns].sum(axis=1)\n",
    "\n",
    "# Avoid division by zero (in case of empty samples)\n",
    "df_avg_composition[element_columns] = df_avg_composition[element_columns].div(total_sum, axis=0) * 100\n",
    "\n",
    "df_heatmap = df_avg_composition.set_index(\"Sample\")\n",
    "\n",
    "# Create heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(df_heatmap, annot=True, fmt=\".1f\", cmap=\"coolwarm\", linewidths=0.5)\n",
    "\n",
    "# Formatting\n",
    "plt.title(\"Atomic Composition Heatmap\")\n",
    "plt.xlabel(\"Element\")\n",
    "plt.ylabel(\"Sample\")\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify unclassified particles where all elements have 0 wt%\n",
    "unclassified_mask = (df_final[element_columns].sum(axis=1) == 0)\n",
    "\n",
    "# Create a 'Cluster' column and set '0' for unclassified particles\n",
    "df_final['Cluster'] = None\n",
    "df_final.loc[unclassified_mask, 'Cluster'] = 0\n",
    "\n",
    "# Filter only classified particles (non-unclassified rows)\n",
    "classified_data = df_final[~unclassified_mask][element_columns]\n",
    "classified_values = classified_data.values\n",
    "#%% Assessing optimal number of clusters\n",
    "\n",
    "UL.K_means_optimal(classified_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_components = 11 # Adjust based on analysis\n",
    "kmeans = KMeans(n_clusters=optimal_components, random_state=42)\n",
    "\n",
    "# Fit K-means\n",
    "cluster_labels = kmeans.fit_predict(classified_values)\n",
    "classified_data['Cluster'] = cluster_labels + 1\n",
    "\n",
    "# Assign the classified clusters back to df\n",
    "df_final.loc[classified_data.index, 'Cluster'] = classified_data['Cluster']\n",
    "\n",
    "df_final['Cluster'] = df_final['Cluster'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots: Top row for solid bar histograms, bottom row for normalized stacked histograms\n",
    "fig, axs = plt.subplots(nrows=2, ncols=num_samples, figsize=(15, 10), sharex=True, sharey=\"row\")\n",
    "\n",
    "# Colormap and cluster color mapping\n",
    "cmap = plt.get_cmap(\"gist_ncar\", optimal_components + 2)\n",
    "cluster_colors = {cluster: cmap(i) for i, cluster in enumerate(sorted(df_final[\"Cluster\"].unique()))}\n",
    "all_clusters = sorted(df_final[\"Cluster\"].unique())  # Ensure all clusters are considered\n",
    "\n",
    "# For use if #optimal_components = 11\n",
    "Cluster_names = [\"Unclassified\", \"Sn rich\", \"Low Fe\", \"High Fe\", \"Si rich\", \"Ca rich\", \"Ni rich\", \"Medium Fe\", \"Zr rich\", \"Ca + S rich\", \"Mg rich\", \"Ti or K rich\"]\n",
    "\n",
    "# Loop through each sample and plot\n",
    "for col, sample in enumerate(samples):\n",
    "    sample_data = df_final[df_final[\"Sample\"] == sample]  # Filter data for the sample\n",
    "\n",
    "    ### Top Row: Solid bar histogram\n",
    "    ax_top = axs[0, col]\n",
    "    # Compute total particle counts per size bin\n",
    "    total_counts, _ = np.histogram(sample_data[\"ECD (Î¼m)\"] * 1000, bins=size_bins)\n",
    "    \n",
    "    total_counts = total_counts/Partector_correction/Fraction_area_analyzed/sampled_air[col]/dlogDP\n",
    "    \n",
    "    ax_top.bar(size_bins[:-1], total_counts, width=np.diff(size_bins), align=\"edge\", color=\"gray\", edgecolor=\"black\")\n",
    "    ax_top.set_title(f\"{sample}, N$_{{total}}$: {len(sample_data)}\",fontsize=12)\n",
    "    ax_top.set_xscale(\"log\")\n",
    "    ax_top.set_xlim(80, 1e4)\n",
    "    ax_top.grid(axis=\"both\", which=\"both\", color=\"k\", alpha=0.2)\n",
    "\n",
    "    ### Bottom Row: Normalized stacked histogram\n",
    "    ax_bottom = axs[1, col]\n",
    "    # Calculate histograms for each cluster\n",
    "    hist_data = []\n",
    "    for cluster in all_clusters:\n",
    "        cluster_data = sample_data[sample_data[\"Cluster\"] == cluster][\"ECD (Î¼m)\"] * 1000\n",
    "        hist_data.append(cluster_data)\n",
    "    \n",
    "    # Stack histograms for clusters\n",
    "    cluster_hist, _ = np.histogram(sample_data[\"ECD (Î¼m)\"] * 1000, bins=size_bins)\n",
    "    cluster_contributions = []\n",
    "    for cluster_data in hist_data:\n",
    "        counts, _ = np.histogram(cluster_data, bins=size_bins)\n",
    "        cluster_contributions.append(counts)\n",
    "\n",
    "    # Normalize stacked histogram to sum to 100% per bin\n",
    "    cluster_contributions = np.array(cluster_contributions)\n",
    "    total_counts = cluster_contributions.sum(axis=0)\n",
    "    normalized_contributions = (cluster_contributions / total_counts) * 100  # Normalize to percentages\n",
    "    normalized_contributions[np.isnan(normalized_contributions)] = 0  # Handle NaNs in empty bins\n",
    "\n",
    "    # Plot normalized stacked histogram\n",
    "    bottom = np.zeros(len(size_bins) - 1)\n",
    "    for i, cluster in enumerate(all_clusters):\n",
    "        ax_bottom.bar(size_bins[:-1], normalized_contributions[i], width=np.diff(size_bins), align=\"edge\",\n",
    "                      bottom=bottom, label=Cluster_names[i], color=cluster_colors[cluster], edgecolor=\"black\")\n",
    "        bottom += normalized_contributions[i]\n",
    "\n",
    "    ax_bottom.set_xscale(\"log\")\n",
    "    ax_bottom.set_xlim(80, 1e4)\n",
    "    ax_bottom.set_ylim(0, 100)\n",
    "    ax_bottom.grid(axis=\"both\", which=\"both\", color=\"k\", alpha=0.2)\n",
    "\n",
    "# Add a shared x-axis label\n",
    "axs[1,2].set_xlabel(\"ECD / nm\")\n",
    "axs[1,0].set_ylabel(\"Contribution / %\")\n",
    "axs[0,0].set_ylabel(\"dN/dlogDp / cm$^{-3}$\",labelpad=15)\n",
    "plt.subplots_adjust(hspace=0.07,wspace=0.15,right=0.85)\n",
    "\n",
    "# Add a legend for clusters (shared across subplots)\n",
    "axs[1, -1].legend(bbox_to_anchor=(1.05, 1.5), loc=\"upper left\", borderaxespad=0)\n",
    "\n",
    "fig.savefig(r\"SEM\\Size_distribution.png\",dpi=600,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the average composition per cluster\n",
    "df_cluster_avg = df_final.groupby(\"Cluster\")[element_columns].mean().reset_index()\n",
    "\n",
    "particle_counts = [np.sum(df_final['Cluster'] == cluster) for cluster in all_clusters]\n",
    "\n",
    "# Set the cluster column as index for plotting\n",
    "df_cluster_avg.set_index(\"Cluster\", inplace=True)\n",
    "\n",
    "# Normalize the atomic percentages so they sum to 100%\n",
    "total_sum_cluster = df_cluster_avg[list(df_cluster_avg)].sum(axis=1)\n",
    "\n",
    "# Avoid division by zero (in case of empty samples)\n",
    "df_cluster_avg[list(df_cluster_avg)] = df_cluster_avg[list(df_cluster_avg)].div(total_sum_cluster, axis=0) * 100\n",
    "\n",
    "# Plot the stacked bar chart\n",
    "ax = df_cluster_avg.plot(kind=\"bar\", stacked=True, figsize=(10, 6), colormap=\"tab20\",edgecolor=\"black\")\n",
    "for i, bar in enumerate(particle_counts):\n",
    "    \n",
    "    x = i\n",
    "    ax.text(\n",
    "            x,\n",
    "            100 + 0.5,  # Position the text slightly above the bar\n",
    "            str(particle_counts[i]),  # Number of particles\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "            fontsize=12,\n",
    "        )\n",
    "# Formatting\n",
    "plt.xlabel(\"Cluster\",fontsize=15)\n",
    "plt.ylabel(\"Atomic Percent / %\",fontsize=15)\n",
    "plt.title(\"Average Composition per Cluster\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\",borderaxespad=0,fontsize=12)\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(r\"SEM\\Cluster_Composition.png\",dpi=300,bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
